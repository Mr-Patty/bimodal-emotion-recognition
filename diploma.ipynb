{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Mr-Patty/bimodal-emotion-recognition\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp drive/'My Drive'/EmotionRecognition/bimodal-emotion-recognition.tar.gz bimodal-emotion-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd bimodal-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C . -xzf bimodal-emotion-recognition.tar.gz\n",
    "!rm bimodal-emotion-recognition.tar.gz\n",
    "!mv Audio_ogg_10 Audio_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import math\n",
    "import librosa\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import librosa.display\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import matplotlib.style as ms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from utils import *\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sampling_rate=44100, wav_files_path='Audio_preprocess/', meta_data=\"df_prep.csv\", shuffle=False):\n",
    "        self.emotion_dict = {'ang': 0,\n",
    "                             'dis': 1,\n",
    "                             'hap': 2,\n",
    "                             'sad': 3,\n",
    "                             'sca': 4,\n",
    "                             'sur': 5,\n",
    "                             'neu': 6\n",
    "                            }\n",
    "        \n",
    "        self.wav_files_path = wav_files_path\n",
    "        self.sr = sampling_rate\n",
    "        self.shuffle = shuffle\n",
    "        self.meta_data = pd.read_csv(meta_data).to_numpy()\n",
    "        \n",
    "    def getAudio(self, file_path):\n",
    "        wav_vector, _sr = librosa.load(file_path, sr=self.sr)\n",
    "#         wav_vector, _sr = torchaudio.load(file_path)\n",
    "        \"\"\"\n",
    "        TODO: implement later needed processing of wav vector\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "#             feature_vector = torchaudio.transforms.MelSpectrogram(_sr)(wav_vector)\n",
    "            feature_vector = librosa.feature.melspectrogram(y=wav_vector, sr=_sr)\n",
    "            feature_vector = torch.from_numpy(feature_vector).permute(1, 0)\n",
    "        except:\n",
    "            print(file_path)\n",
    "            raise\n",
    "        return feature_vector\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.wav_files_path + self.meta_data[index][0] + '.ogg'\n",
    "        emotion = self.emotion_dict[self.meta_data[index][1]]\n",
    "        emotion = torch.tensor(emotion)\n",
    "        return self.getAudio(file_path), emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(wav_files_path='Audio_ogg_10/')\n",
    "train_loader = DataLoader(dataset, num_workers=1, shuffle=False,\n",
    "                              batch_size=1)\n",
    "\n",
    "# for batch in tqdm(train_loader):\n",
    "#     x, y = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNNModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, embedding_length, bidirectional=False, num_layers=1, dropout=0.5):\n",
    "        super(AttentionRNNModel, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "#         self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size, bidirectional=self.bidirectional, num_layers=self.num_layers)\n",
    "        self.label = nn.Linear(hidden_size * 2 if self.bidirectional else hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.attn_fc_layer = nn.Linear()\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "\n",
    "        \"\"\" \n",
    "        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
    "        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
    "        final_state : Final time-step hidden state (h_n) of the LSTM\n",
    "\n",
    "        ---------\n",
    "    \n",
    "        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
    "                  new hidden state.\n",
    "\n",
    "        Tensor Size :\n",
    "                hidden.size() = (batch_size, hidden_size)\n",
    "                attn_weights.size() = (batch_size, num_seq)\n",
    "                soft_attn_weights.size() = (batch_size, num_seq)\n",
    "                new_hidden_state.size() = (batch_size, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, input, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        # h_0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        # c_0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
    "        # output = output.permute(0, 2, 1) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "\n",
    "        # attn_output = self.attention_net(output, final_hidden_state)\n",
    "        final_hidden_state = self.dropout(torch.cat((final_hidden_state[-2,:,:], final_hidden_state[-1,:,:]), dim = 1))\n",
    "        logits = self.label(final_hidden_state)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "def Hybrid_model(nn.Module):\n",
    "    def __init__(self, model_audio, model_text, hidden_dim=64, output_dim=7):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model_audio = model_audio\n",
    "        self.model_text = model_text\n",
    "        self.model_text.fc = nn.Identity()\n",
    "        self.model_audio.label = nn.Identity()\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, mel):\n",
    "        \n",
    "        hid_text = self.model_text(text, text_lengths)\n",
    "        hid_audio = self.model_audio(mel)\n",
    "        \n",
    "        hid_context = torch.cat((hid_text, hid_audio), dim = 1)\n",
    "        print(hid_context.shape)\n",
    "        \n",
    "        return self.fc(hid_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        file_path = batch.file\n",
    "        melpath = 'Train_data/' + file_path + '.mel'\n",
    "        mel = torch.load(melpath).squeeze(0).permute(1, 0)\n",
    "        \n",
    "        predictions = model(text, text_lengths, mel)\n",
    "#         print(predictions.shape, batch.label.shape)\n",
    "        # print(predictions, batch.label)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            file_path = batch.file\n",
    "            melpath = 'Train_data/' + file_path + '.mel'\n",
    "            mel = torch.load(melpath).squeeze(0).permute(1, 0)\n",
    "            \n",
    "            predictions = model(text, text_lengths, mel).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ru import Russian\n",
    "from spacy_russian_tokenizer import RussianTokenizer, MERGE_PATTERNS\n",
    "nlp = Russian()\n",
    "russian_tokenizer = RussianTokenizer(nlp, MERGE_PATTERNS)\n",
    "nlp.add_pipe(russian_tokenizer, name='russian_tokenizer')\n",
    "def tokenize_ru(sentence):\n",
    "    return [tok.text for tok in nlp(sentence)]\n",
    "\n",
    "FILE = data.Field()\n",
    "TEXT = data.Field(tokenize = tokenize_ru, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "fields = [('file', FILE), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = './',\n",
    "                                        train = 'train_text.csv',\n",
    "                                        # validation = 'valid.csv',\n",
    "                                        test = 'test_text.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "import os\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ru.vec'\n",
    "name = os.path.basename(url)\n",
    "vec = torchtext.vocab.Vectors(name, url=url)\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# vec = torchtext.vocab.FastText(language='en')\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = vec, \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "FILE.build_vocab(train_data)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    sort = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = 1\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_audio = AttentionRNNModel(batch_size=Batch, output_size=7, hidden_size=64, embedding_length=128)\n",
    "model_text = TextRNN(INPUT_DIM, \n",
    "                    EMBEDDING_DIM, \n",
    "                    HIDDEN_DIM, \n",
    "                    OUTPUT_DIM, \n",
    "                    N_LAYERS, \n",
    "                    BIDIRECTIONAL, \n",
    "                    DROPOUT, \n",
    "                    PAD_IDX)\n",
    "# pretrained_embeddings = TEXT.vocab.vectors\n",
    "# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "# model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "model_audio.load_state_dict('audio_model.pt')\n",
    "model_text.load_state_dict('text_model.pt')\n",
    "for param in model_audio.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model_text.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model = Hybrid_model(model_audio, model_text, hidden_dim=64, output_dim=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acces = []\n",
    "test_acces = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    train_acces.append(train_acc)\n",
    "    test_acces.append(valid_acc)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'hybrid-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp hybrid-model.pt drive/'My Drive'/EmotionRecognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(14 ,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Train loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14 ,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(test_losses)\n",
    "plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14 ,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(train_acces)\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14 ,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(test_acces)\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train_loss, val_loss, best_model = train(model, dataset, device, batch_size=Batch, n_epoch=1, validation_split=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, num_workers=1, shuffle=False,\n",
    "                              batch_size=1)\n",
    "\n",
    "for batch in dataset:\n",
    "    x, y= batch\n",
    "    if x.size(0) == 0:\n",
    "#         print(x.shape, y.shape)\n",
    "        print(x)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
