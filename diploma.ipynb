{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://username:password@github.com/Mr-Patty/bimodal-emotion-recognition\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp drive/'My Drive'/EmotionRecognition/speechRecognition.tar.gz bimodal-emotion-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd bimodal-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C . -xzf speechRecognition.tar.gz\n",
    "!rm Audio/mic_2_failed.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import math\n",
    "import librosa\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import librosa.display\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import matplotlib.style as ms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from utils import *\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sampling_rate=44100, wav_files_path='Audio_preprocess/', meta_data=\"df_prep.csv\", shuffle=False):\n",
    "        self.emotion_dict = {'ang': 0,\n",
    "                             'dis': 1,\n",
    "                             'hap': 2,\n",
    "                             'sad': 3,\n",
    "                             'sca': 4,\n",
    "                             'sur': 5,\n",
    "                             'neu': 6\n",
    "                            }\n",
    "        \n",
    "        self.wav_files_path = wav_files_path\n",
    "        self.sr = sampling_rate\n",
    "        self.shuffle = shuffle\n",
    "        self.meta_data = pd.read_csv(meta_data).to_numpy()\n",
    "        \n",
    "    def getAudio(self, file_path):\n",
    "        wav_vector, _sr = librosa.load(file_path, sr=self.sr)\n",
    "#         wav_vector, _sr = torchaudio.load(file_path)\n",
    "        \"\"\"\n",
    "        TODO: implement later needed processing of wav vector\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "#             feature_vector = torchaudio.transforms.MelSpectrogram(_sr)(wav_vector)\n",
    "            feature_vector = librosa.feature.melspectrogram(y=wav_vector, sr=_sr)\n",
    "            feature_vector = torch.from_numpy(feature_vector).permute(1, 0)\n",
    "        except:\n",
    "            print(file_path)\n",
    "            raise\n",
    "        return feature_vector\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.meta_data[index][0]\n",
    "        emotion = self.emotion_dict[self.meta_data[index][1]]\n",
    "        emotion = torch.tensor(emotion)\n",
    "        return self.getAudio(file_path), emotion\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset()\n",
    "train_loader = DataLoader(dataset, num_workers=1, shuffle=False,\n",
    "                              batch_size=1)\n",
    "\n",
    "for batch in dataset:\n",
    "    x, y= batch\n",
    "    if x.size(0) == 0:\n",
    "#         print(x.shape, y.shape)\n",
    "        print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"docstring for LSTMClassifier\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.input_dim = config['input_dim']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.output_dim = config['output_dim']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "        self.dropout = config['dropout'] if self.n_layers > 1 else 0\n",
    "\n",
    "        self.rnn = nn.LSTM(self.input_dim, self.hidden_dim, bias=True,\n",
    "                           num_layers=1, dropout=self.dropout,\n",
    "                           bidirectional=self.bidirectional)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.softmax = F.softmax\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # input_seq =. [1, batch_size, input_size]\n",
    "        rnn_output, (hidden, _) = self.rnn(input_seq)\n",
    "        if self.bidirectional:  # sum outputs from the two directions\n",
    "            rnn_output = rnn_output[:, :, :self.hidden_dim] +\\\n",
    "                        rnn_output[:, :, self.hidden_dim:]\n",
    "        out = self.out(rnn_output[0])\n",
    "        print(out.shape)\n",
    "        class_scores = F.softmax(out) \n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FCNAttention(torch.nn.Module):\n",
    "#     def __init__(self, batch_size, output_size):\n",
    "#         super(FCNAttention, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, embedding_length, bidirectional=False, num_layers=1):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "#         self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size, bidirectional=self.bidirectional, num_layers=self.num_layers)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        #self.attn_fc_layer = nn.Linear()\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "\n",
    "        \"\"\" \n",
    "        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
    "        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
    "        final_state : Final time-step hidden state (h_n) of the LSTM\n",
    "\n",
    "        ---------\n",
    "    \n",
    "        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
    "                  new hidden state.\n",
    "\n",
    "        Tensor Size :\n",
    "                hidden.size() = (batch_size, hidden_size)\n",
    "                attn_weights.size() = (batch_size, num_seq)\n",
    "                soft_attn_weights.size() = (batch_size, num_seq)\n",
    "                new_hidden_state.size() = (batch_size, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, input, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2).float()\n",
    "        if batch_size is None:\n",
    "            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).float())\n",
    "            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).float())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).float())\n",
    "            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).float())\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
    "        output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "\n",
    "        attn_output = self.attention_net(output, final_hidden_state)\n",
    "        logits = self.label(attn_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'gpu': 0,\n",
    "#     'bidirectional': False,\n",
    "#     'input_dim': 128,\n",
    "#     'hidden_dim': 50,\n",
    "#     'output_dim': 7,  # number of classes\n",
    "#     'dropout': 0.2,\n",
    "#     'learning_rate': 0.001,\n",
    "#     'batch_size': 1,  # carefully chosen\n",
    "#     'n_epochs': 55000,\n",
    "#     'n_layers': 2,\n",
    "#     'model_code': 'basic_lstm'\n",
    "# }\n",
    "# model = LSTMClassifier(config)\n",
    "\n",
    "model = AttentionModel(batch_size=1, output_size=7, hidden_size=32, embedding_length=128)\n",
    "dataset = AudioDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Loading data\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/512 [00:03<05:38,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interrupt, continue work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train(model, dataset, device, batch_size=1, n_epoch=1, validation_split=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, num_workers=1, shuffle=False,\n",
    "                              batch_size=1)\n",
    "\n",
    "for batch in dataset:\n",
    "    x, y= batch\n",
    "    if x.size(0) == 0:\n",
    "#         print(x.shape, y.shape)\n",
    "        print(x)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
