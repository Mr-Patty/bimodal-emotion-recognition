{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import math\n",
    "import librosa\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import librosa.display\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import matplotlib.style as ms\n",
    "from tqdm import tqdm\n",
    "import IPython.display\n",
    "import random\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = \"Audio/\"\n",
    "audio_preproccess = 'data/Audio_pre/'\n",
    "annotations_data_by_emotions = \"Annotations_by_emotions/\"\n",
    "\n",
    "df_angry = pd.read_csv(annotations_data_by_emotions + 'data_Angry.csv', index_col='ID')\n",
    "df_disgusted = pd.read_csv(annotations_data_by_emotions + 'data_Disgusted.csv', index_col='ID')\n",
    "df_happy = pd.read_csv(annotations_data_by_emotions + 'data_Happy.csv', index_col='ID')\n",
    "df_neutral = pd.read_csv(annotations_data_by_emotions + 'data_Neutral.csv', index_col='ID')\n",
    "df_sad = pd.read_csv(annotations_data_by_emotions + 'data_Sad.csv', index_col='ID')\n",
    "df_scared = pd.read_csv(annotations_data_by_emotions + 'data_Scared.csv', index_col='ID')\n",
    "df_surprised = pd.read_csv(annotations_data_by_emotions + 'data_Surprised.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_angry['Emotion'] = 'ang'\n",
    "df_disgusted['Emotion'] = 'dis' \n",
    "df_happy['Emotion'] = 'hap' \n",
    "df_neutral['Emotion'] = 'neu' \n",
    "df_sad['Emotion'] = 'sad'\n",
    "df_scared['Emotion'] = 'sca' \n",
    "df_surprised['Emotion'] = 'sur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([df_happy, df_angry, df_disgusted, df_neutral, df_sad, df_scared, df_surprised])\n",
    "df_complete.File += '_mic.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_csv('df_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('df_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "audio_vectors = {}\n",
    "metadata_vectors = {}\n",
    "for sess in [1]:  # using one session due to memory constraint, can replace [5] with range(1, 6)\n",
    "    wav_file_path = 'Audio/'\n",
    "    orig_wav_files = os.listdir(wav_file_path)\n",
    "    for orig_wav_file in tqdm(orig_wav_files):\n",
    "        try:\n",
    "            orig_wav_vector, _sr = librosa.load(wav_file_path + orig_wav_file, sr=sr)\n",
    "            orig_wav_file, file_format = orig_wav_file.split('.')\n",
    "            for index, row in labels_df[labels_df['File'].str.contains(orig_wav_file)].iterrows():\n",
    "                start_time, end_time, truncated_wav_file_name, emotion = row['Start'], row['End'], row['File'], row['Emotion']\n",
    "                start_frame = math.floor(start_time * sr)\n",
    "                end_frame = math.floor(end_time * sr)\n",
    "                truncated_wav_vector = orig_wav_vector[start_frame:end_frame + 1]\n",
    "                audio_vectors[\"{}_\".format(index) + truncated_wav_file_name] = (truncated_wav_vector, emotion)\n",
    "#                 metadata_vectors[\"{}_\".format(index) + truncated_wav_file_name] = emotion\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except:\n",
    "            print('An exception occured for {}'.format(orig_wav_file))\n",
    "#     with open('audio_vectors/audio_vectors_{}.pkl'.format(sess), 'wb') as f:\n",
    "#         pickle.dump(audio_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_preproccess = \"Audio_preprocess/\"\n",
    "# sf.write('stereo_file.wav', np.random.randn(10, 2), 44100, 'PCM_24')\n",
    "files = []\n",
    "emotions = []\n",
    "for i in tqdm(audio_vectors):\n",
    "    files.append(path_preproccess + i)\n",
    "    emotions.append(audio_vectors[i][1])\n",
    "    sf.write(path_preproccess + i, audio_vectors[i][0], 44100, 'PCM_24')\n",
    "preprop_data = {\"File\": files, \"Emotion\" : emotions}\n",
    "df_prep = pd.DataFrame.from_dict(preprop_data)\n",
    "df_prep.to_csv('df_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('Audio_preprocess/3281_22dec_D62_1_mic.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['10dec_D11_1_mic.wav', 104.75, 116.5, 'hap'],\n",
       "       ['10dec_D11_1_mic.wav', 199.3, 210.22, 'hap'],\n",
       "       ['10dec_D42_1_mic.wav', 27.153, 29.44, 'hap'],\n",
       "       ...,\n",
       "       ['22dec_K42_1_mic.wav', 25.02, 30.32, 'sur'],\n",
       "       ['22dec_K43_2_mic.wav', 6.72, 20.27, 'sur'],\n",
       "       ['22dec_K44_2_mic.wav', 18.227, 23.67, 'sur']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
