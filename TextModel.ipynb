{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkC2QJ-w4DMV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torchtext\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "oVJ6Cppk91mL",
    "outputId": "c1e3fb76-0a12-4a19-8205-c1fede985837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKX_vyyiAmIT"
   },
   "outputs": [],
   "source": [
    "!cp drive/'My Drive'/EmotionRecognition/train_text.csv .\n",
    "!cp drive/'My Drive'/EmotionRecognition/test_text.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "colab_type": "code",
    "id": "ABMDblGcBZkw",
    "outputId": "4be31486-1c48-43bd-aa28-de45ab249fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
      "  Cloning https://github.com/aatimofeev/spacy_russian_tokenizer.git to /tmp/pip-req-build-hji4ao6v\n",
      "  Running command git clone -q https://github.com/aatimofeev/spacy_russian_tokenizer.git /tmp/pip-req-build-hji4ao6v\n",
      "Requirement already satisfied (use --upgrade to upgrade): spacy-russian-tokenizer==0.1.1 from git+https://github.com/aatimofeev/spacy_russian_tokenizer.git in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from spacy-russian-tokenizer==0.1.1) (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (2.23.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (4.41.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (46.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.18.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->spacy-russian-tokenizer==0.1.1) (1.1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->spacy-russian-tokenizer==0.1.1) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->spacy-russian-tokenizer==0.1.1) (3.1.0)\n",
      "Building wheels for collected packages: spacy-russian-tokenizer\n",
      "  Building wheel for spacy-russian-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spacy-russian-tokenizer: filename=spacy_russian_tokenizer-0.1.1-cp36-none-any.whl size=12675 sha256=350dec57d0c0bcbabab664bb4a3e17ffd59ea1d7d9e91dc69afbb62881dee5e3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wvt50pac/wheels/37/3b/bb/cfe712f7c0b78cd08f4a2ef122d17748baf9d4bebecf2e5a54\n",
      "Successfully built spacy-russian-tokenizer\n",
      "Collecting pymorphy2==0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8) (0.6.2)\n",
      "Collecting dawg-python>=0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 8.0MB/s \n",
      "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aatimofeev/spacy_russian_tokenizer.git\n",
    "!pip install pymorphy2==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtSF0ptb5Yur"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.ru import Russian\n",
    "from spacy_russian_tokenizer import RussianTokenizer, MERGE_PATTERNS\n",
    "nlp = Russian()\n",
    "russian_tokenizer = RussianTokenizer(nlp, MERGE_PATTERNS)\n",
    "nlp.add_pipe(russian_tokenizer, name='russian_tokenizer')\n",
    "def tokenize_ru(sentence):\n",
    "    return [tok.text for tok in nlp(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlmFstGH5dTS"
   },
   "outputs": [],
   "source": [
    "FILE = data.Field()\n",
    "TEXT = data.Field(tokenize = tokenize_ru, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50nsM54089by"
   },
   "outputs": [],
   "source": [
    "fields = [('file', FILE), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "                                        path = './',\n",
    "                                        train = 'train_text.csv',\n",
    "                                        # validation = 'valid.csv',\n",
    "                                        test = 'test_text.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2qwUasfSaXO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfckkmfQ8s4t"
   },
   "outputs": [],
   "source": [
    "# train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "# train_data, test_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "7MlVcBum8231",
    "outputId": "610b575a-2c16-4cf8-ddeb-4b85d0cd65f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7158\n",
      "Number of testing examples: 3069\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "# print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDFp2kPJDtJk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ru.vec'\n",
    "name = os.path.basename(url)\n",
    "vec = torchtext.vocab.Vectors(name, url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "BFUFlS4o_Wrs",
    "outputId": "bc30f527-3d4d-4d32-c54f-a472e2ab480a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 3614\n",
      "Unique tokens in LABEL vocabulary: 3\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "# vec = torchtext.vocab.FastText(language='en')\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = vec, \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "FILE.build_vocab(train_data)\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAPKfINLIPum"
   },
   "outputs": [],
   "source": [
    "# len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "TBlBJYLT_aMZ",
    "outputId": "6dba9dae-50f4-4415-aa50-b1e894f06d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    sort = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ukFT7YLk7kz5"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfnivaOO_c0x"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6hnIZP8M7tJk",
    "outputId": "8eaad22e-893e-4e93-c436-151292892b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3614, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "D67H2eJBPypC",
    "outputId": "a97931d7-883e-45fc-eae4-b70aa1e9ff58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0438,  0.1425,  0.1500,  ...,  0.0027, -0.0461,  0.0745],\n",
       "        [ 0.5091, -0.2560, -0.6233,  ...,  0.0328, -0.0662,  0.0265],\n",
       "        [ 0.3036, -0.6419, -0.7598,  ...,  0.2153, -0.3162,  0.2115]])"
      ]
     },
     "execution_count": 194,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "fBTJcdB_P2mg",
    "outputId": "814a4d26-2f47-460a-8348-7f430a002c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0438,  0.1425,  0.1500,  ...,  0.0027, -0.0461,  0.0745],\n",
      "        [ 0.5091, -0.2560, -0.6233,  ...,  0.0328, -0.0662,  0.0265],\n",
      "        [ 0.3036, -0.6419, -0.7598,  ...,  0.2153, -0.3162,  0.2115]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GNDiv-c_o2o"
   },
   "outputs": [],
   "source": [
    "# def binary_accuracy(preds, y):\n",
    "#     \"\"\"\n",
    "#     Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "#     \"\"\"\n",
    "\n",
    "#     #round predictions to the closest integer\n",
    "#     rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "#     correct = (rounded_preds == y).float() #convert into float for division \n",
    "#     acc = correct.sum() / len(correct)\n",
    "#     return acc\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8emHpqmIfChG"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OtfsjxyK_0QA",
    "outputId": "48343244-e3a9-401b-cefd-1bbfff3ea491"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.516 | Train Acc: 74.43%\n",
      "\t Val. Loss: 1.677 |  Val. Acc: 58.80%\n",
      "LR: [0.001]\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.515 | Train Acc: 74.41%\n",
      "\t Val. Loss: 1.810 |  Val. Acc: 58.88%\n",
      "LR: [0.001]\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.516 | Train Acc: 74.45%\n",
      "\t Val. Loss: 1.786 |  Val. Acc: 58.12%\n",
      "LR: [0.001]\n",
      "Epoch: 04 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.516 | Train Acc: 74.41%\n",
      "\t Val. Loss: 1.740 |  Val. Acc: 59.11%\n",
      "LR: [0.001]\n",
      "Epoch: 05 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.519 | Train Acc: 74.26%\n",
      "\t Val. Loss: 1.763 |  Val. Acc: 59.20%\n",
      "LR: [0.001]\n",
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.519 | Train Acc: 74.25%\n",
      "\t Val. Loss: 1.633 |  Val. Acc: 58.93%\n",
      "LR: [0.001]\n",
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.517 | Train Acc: 74.14%\n",
      "\t Val. Loss: 1.649 |  Val. Acc: 58.54%\n",
      "LR: [0.001]\n",
      "Epoch: 08 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.514 | Train Acc: 74.40%\n",
      "\t Val. Loss: 1.676 |  Val. Acc: 58.96%\n",
      "LR: [0.001]\n",
      "Epoch: 09 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.515 | Train Acc: 74.37%\n",
      "\t Val. Loss: 1.733 |  Val. Acc: 59.08%\n",
      "LR: [0.001]\n",
      "Epoch: 10 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.516 | Train Acc: 74.30%\n",
      "\t Val. Loss: 1.675 |  Val. Acc: 58.84%\n",
      "LR: [0.001]\n",
      "Epoch: 11 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.513 | Train Acc: 74.50%\n",
      "\t Val. Loss: 1.789 |  Val. Acc: 58.98%\n",
      "LR: [0.001]\n",
      "Epoch: 12 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.518 | Train Acc: 74.17%\n",
      "\t Val. Loss: 1.693 |  Val. Acc: 58.43%\n",
      "LR: [0.001]\n",
      "Epoch: 13 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.515 | Train Acc: 74.32%\n",
      "\t Val. Loss: 1.813 |  Val. Acc: 58.81%\n",
      "LR: [0.001]\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 500\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acces = []\n",
    "test_acces = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "    train_acces.append(train_acc)\n",
    "    test_acces.append(valid_acc)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'text-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print(f'LR:', scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecxwTskCTzcW"
   },
   "outputs": [],
   "source": [
    "!cp text-model.pt drive/'My Drive'/EmotionRecognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGENlvHMAUvR"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('text-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3E6bUX5gbCV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
