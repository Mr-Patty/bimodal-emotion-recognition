{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Mr-Patty/bimodal-emotion-recognition\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp drive/'My Drive'/EmotionRecognition/bimodal-emotion-recognition.tar.gz bimodal-emotion-recognition/\n",
    "%cd bimodal-emotion-recognition\n",
    "!cp drive/'My Drive'/EmotionRecognition/df_prep.csv .\n",
    "!tar -C . -xzf bimodal-emotion-recognition.tar.gz\n",
    "!rm bimodal-emotion-recognition.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.hparams import HParam\n",
    "from datasets.dataloader import create_dataloader, create_dataset\n",
    "\n",
    "from utils.utils import read_wav_np\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import math\n",
    "import librosa\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import librosa.display\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import matplotlib.style as ms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from utils.train import *\n",
    "from models import *\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HParam('config/default.yaml')\n",
    "with open('config/default.yaml', 'r') as f:\n",
    "    hp_str = ''.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dir = os.path.join(hp.log.chkpt_dir, 'AudioModel')\n",
    "log_dir = os.path.join(hp.log.log_dir, 'AudioModel')\n",
    "os.makedirs(pt_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir,\n",
    "            '%s-%d.log' % ('AudioModel', time.time()))),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hp.audio.hop_length == 256, \\\n",
    "    'hp.audio.hop_length must be equal to 256, got %d' % hp.audio.hop_length\n",
    "assert hp.data.train != '' \\\n",
    "    'hp.data.train and hp.data.validation can\\'t be empty: please fix %s' % 'config/default.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = create_dataset(hp, 'df_prep.csv', True)\n",
    "# train_loader = DataLoader(dataset, num_workers=1, shuffle=False,\n",
    "#                               batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(trainloader):\n",
    "#     print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = 1\n",
    "model = AttentionRNNModel(batch_size=Batch, output_size=7, hidden_size=32, embedding_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "train_loss, val_loss, best_model = train(model, trainset, device, batch_size=Batch, n_epoch=1, validation_split=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
