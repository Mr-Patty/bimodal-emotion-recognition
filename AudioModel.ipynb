{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "xZH9sEiaUN1g",
    "outputId": "f581933b-f4be-4b4a-9400-355ac6c13e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bimodal-emotion-recognition'...\n",
      "remote: Enumerating objects: 103, done.\u001b[K\n",
      "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
      "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
      "remote: Total 103 (delta 44), reused 71 (delta 15), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (103/103), 3.12 MiB | 15.82 MiB/s, done.\n",
      "Resolving deltas: 100% (44/44), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Mr-Patty/bimodal-emotion-recognition\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "MxdVtSXbUN1t",
    "outputId": "09456bb2-84c7-4bd8-c1a8-3c4fb1b89866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "2z6pKQ0qUN1-",
    "outputId": "ac9e9605-021e-41bd-9e94-328f26f2725f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/bimodal-emotion-recognition\n"
     ]
    }
   ],
   "source": [
    "!cp drive/'My Drive'/EmotionRecognition/bimodal-emotion-recognition.tar.gz bimodal-emotion-recognition/\n",
    "%cd bimodal-emotion-recognition\n",
    "!cp ../drive/'My Drive'/EmotionRecognition/df_prep.csv .\n",
    "!tar -C . -xzf bimodal-emotion-recognition.tar.gz\n",
    "!rm bimodal-emotion-recognition.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "VDE2n05TdwIS",
    "outputId": "80ae8cff-66ae-4601-91fa-f253da032424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/bimodal-emotion-recognition\n"
     ]
    }
   ],
   "source": [
    "%cd bimodal-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "p-3VOHmIXWZ_",
    "outputId": "f3f37655-cb4c-4340-f3e9-179b2832a09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioModel.ipynb  diploma.ipynb        preprocess.py\tTrain_data\n",
      "chkpt\t\t  experiment.ipynb     processing.py\tutils\n",
      "config\t\t  logs\t\t       README.md\tutils.py\n",
      "datasets\t  models.py\t       test.png\n",
      "df_prep.csv\t  Preprocessing.ipynb  TextModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNbfCWoFUN2I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.hparams import HParam\n",
    "from datasets.dataloader import create_dataloader, create_dataset\n",
    "\n",
    "from utils.utils import read_wav_np\n",
    "import wave\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import math\n",
    "import librosa\n",
    "import json\n",
    "import wave\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import librosa.display\n",
    "import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import librosa.display\n",
    "# import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import matplotlib.style as ms\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from utils.train import *\n",
    "# from models import *\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZRZCnuSUN2Q"
   },
   "outputs": [],
   "source": [
    "hp = HParam('config/default.yaml')\n",
    "with open('config/default.yaml', 'r') as f:\n",
    "    hp_str = ''.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qitik1qIUN2Y"
   },
   "outputs": [],
   "source": [
    "pt_dir = os.path.join(hp.log.chkpt_dir, 'AudioModel')\n",
    "log_dir = os.path.join(hp.log.log_dir, 'AudioModel')\n",
    "os.makedirs(pt_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZuJ-z-yUN2h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(log_dir,\n",
    "            '%s-%d.log' % ('AudioModel', time.time()))),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afrnQ2NRUN2n"
   },
   "outputs": [],
   "source": [
    "assert hp.audio.hop_length == 256, \\\n",
    "    'hp.audio.hop_length must be equal to 256, got %d' % hp.audio.hop_length\n",
    "assert hp.data.train != '' \\\n",
    "    'hp.data.train and hp.data.validation can\\'t be empty: please fix %s' % 'config/default.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDy1vLAAUN20"
   },
   "outputs": [],
   "source": [
    "trainset = create_dataset(hp, 'df_prep.csv', True)\n",
    "train_loader = DataLoader(trainset, num_workers=1, shuffle=False,\n",
    "                              batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n = 0\n",
    "for i in tqdm(train_loader):\n",
    "    x = i[0]\n",
    "#     print(x.shape)\n",
    "#     if x.shape[1] > min_n:\n",
    "#         min_n = x.shape[1]\n",
    "# min_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOuZ7TbW6SLZ"
   },
   "outputs": [],
   "source": [
    "class AttentionRNNModel(torch.nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, embedding_length, bidirectional=False, num_layers=1, dropout=0.5):\n",
    "        super(AttentionRNNModel, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "#         self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "#         self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "        self.lstm = nn.LSTM(embedding_length, hidden_size, bidirectional=self.bidirectional, num_layers=self.num_layers)\n",
    "        self.label = nn.Linear(hidden_size * 2 if self.bidirectional else hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.attn_fc_layer = nn.Linear()\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "\n",
    "        \"\"\" \n",
    "        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n",
    "        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n",
    "        final_state : Final time-step hidden state (h_n) of the LSTM\n",
    "\n",
    "        ---------\n",
    "    \n",
    "        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n",
    "                  new hidden state.\n",
    "\n",
    "        Tensor Size :\n",
    "                hidden.size() = (batch_size, hidden_size)\n",
    "                attn_weights.size() = (batch_size, num_seq)\n",
    "                soft_attn_weights.size() = (batch_size, num_seq)\n",
    "                new_hidden_state.size() = (batch_size, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, input, batch_size=None):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n",
    "        final_output.shape = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        # h_0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "        # c_0 = torch.zeros(1, self.batch_size, self.hidden_size)\n",
    "\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
    "        # output = output.permute(0, 2, 1) # output.size() = (batch_size, num_seq, hidden_size)\n",
    "\n",
    "        # attn_output = self.attention_net(output, final_hidden_state)\n",
    "        final_hidden_state = self.dropout(torch.cat((final_hidden_state[-2,:,:], final_hidden_state[-1,:,:]), dim = 1))\n",
    "        logits = self.label(final_hidden_state)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyQmro3dUN3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcwORjhqcTT8"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=hp.train.batch_size, shuffle=True,\n",
    "#             num_workers=hp.train.num_workers, pin_memory=True, drop_last=False)\n",
    "# model = model.to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# for x, y in train_loader:\n",
    "#     # y_pred = model(x)\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     y_pred = model(x)\n",
    "#     loss = loss_func(y_pred, y)\n",
    "#     print(y_pred.shape)\n",
    "#     print(y)\n",
    "#     # print(x.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "QANdSkkYUN3I",
    "outputId": "b88c00c5-ada1-4344-a07d-3a765d4d3842"
   },
   "outputs": [],
   "source": [
    "Batch = 1\n",
    "model = AttentionRNNModel(batch_size=Batch, output_size=7, hidden_size=32, embedding_length=80, bidirectional=True)\n",
    "trainset = create_dataset(hp, 'df_prep.csv', True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loss, val_loss, train_acces, test_acces, best_model = train(model, trainset, device, batch_size=Batch, n_epoch=50, validation_split=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-bD7EQ4GODo"
   },
   "outputs": [],
   "source": [
    "!cp audio-model.pt drive/'My Drive'/EmotionRecognition/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AudioModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
